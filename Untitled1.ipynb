{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# Importing a few others I want\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = 'train.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 39209\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value.\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = len(X_test)\n",
    "\n",
    "# TODO: What's the shape of a traffic sign image?\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = np.unique(y_train).size\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  210.,  2220.,  2250.,  1410.,  1980.,  1860.,   420.,  1440.,\n",
       "         1410.,  1470.,  2010.,  1320.,  2100.,  2160.,   780.,   630.,\n",
       "          420.,  1110.,  1200.,   210.,   360.,   330.,   390.,   510.,\n",
       "          270.,  1500.,   600.,   240.,   540.,   270.,   450.,   780.,\n",
       "          240.,   689.,   420.,  1200.,   390.,   210.,  2070.,   300.,\n",
       "          360.,   240.,   240.]),\n",
       " array([  0.        ,   0.97674419,   1.95348837,   2.93023256,\n",
       "          3.90697674,   4.88372093,   5.86046512,   6.8372093 ,\n",
       "          7.81395349,   8.79069767,   9.76744186,  10.74418605,\n",
       "         11.72093023,  12.69767442,  13.6744186 ,  14.65116279,\n",
       "         15.62790698,  16.60465116,  17.58139535,  18.55813953,\n",
       "         19.53488372,  20.51162791,  21.48837209,  22.46511628,\n",
       "         23.44186047,  24.41860465,  25.39534884,  26.37209302,\n",
       "         27.34883721,  28.3255814 ,  29.30232558,  30.27906977,\n",
       "         31.25581395,  32.23255814,  33.20930233,  34.18604651,\n",
       "         35.1627907 ,  36.13953488,  37.11627907,  38.09302326,\n",
       "         39.06976744,  40.04651163,  41.02325581,  42.        ]),\n",
       " <a list of 43 Patch objects>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127e25cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Data exploration visualization goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# Making a histogram for the distribution\n",
    "plt.hist(y_train, bins = n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Grayscales an image\n",
    "def grayscale(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "# Normalizes the data between 0.1 and 0.9 instead of 0 to 255    \n",
    "def normalize(data):\n",
    "    return data / 255 * 0.8 + 0.1\n",
    "\n",
    "# Iterates through grayscale for each image in the data\n",
    "def preprocess(data):\n",
    "    gray_images = []\n",
    "    for image in data:\n",
    "        gray = grayscale(image)\n",
    "        gray_images.append(gray)\n",
    "        \n",
    "    return np.array(gray_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Finished preprocessing training data.\n",
      "Processed training data shape = (39209, 32, 32, 1)\n",
      "Preprocessing testing data...\n",
      "Finished preprocessing testing data.\n",
      "Processed testing data shape = (12630, 32, 32, 1)\n",
      "All data preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "from numpy import newaxis\n",
    "\n",
    "print('Preprocessing training data...')\n",
    "\n",
    "# Iterate through grayscale\n",
    "X_train = preprocess(X_train)\n",
    "X_train = X_train[..., newaxis]\n",
    "\n",
    "# Normalize\n",
    "X_train = normalize(X_train) \n",
    "\n",
    "print('Finished preprocessing training data.')\n",
    "\n",
    "# Double-check that the image is changed to depth of 1\n",
    "image_shape2 = X_train.shape\n",
    "print(\"Processed training data shape =\", image_shape2)\n",
    "\n",
    "print('Preprocessing testing data...')\n",
    "\n",
    "# Iterate through grayscale\n",
    "X_test = preprocess(X_test)\n",
    "X_test = X_test[..., newaxis]\n",
    "\n",
    "# Normalize\n",
    "X_test = normalize(X_test) \n",
    "\n",
    "print('Finished preprocessing testing data.')\n",
    "\n",
    "# Double-check that the image is changed to depth of 1\n",
    "image_shape3 = X_test.shape\n",
    "print(\"Processed testing data shape =\", image_shape3)\n",
    "\n",
    "print('All data preprocessing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating new data.\n",
      "Additional data generated. Any classes lacking data now have 911 pictures.\n"
     ]
    }
   ],
   "source": [
    "### Generate additional data (OPTIONAL!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# I will generate some additional data, then split the data in a later cell.\n",
    "# This is to help with the issue identified in the original histogram\n",
    "from scipy import ndimage\n",
    "import random\n",
    "\n",
    "# min_desired below is just mean_pics but wanted to make the code below easier to distinguish\n",
    "pics_in_class = np.bincount(y_train)\n",
    "mean_pics = int(np.mean(pics_in_class))\n",
    "min_desired = int(mean_pics)\n",
    "\n",
    "print('Generating new data.')\n",
    "\n",
    "# Angles to be used to rotate images in additional data made\n",
    "angles = [-10, 10, -15, 15, -20, 20]\n",
    "\n",
    "# Iterate through each class\n",
    "for i in range(len(pics_in_class)):\n",
    "    \n",
    "    # Check if less data than the mean\n",
    "    if pics_in_class[i] < min_desired:\n",
    "        \n",
    "        # Count how many additional pictures we want\n",
    "        new_wanted = min_desired - pics_in_class[i]\n",
    "        picture = np.where(y_train == i)\n",
    "        more_X = []\n",
    "        more_y = []\n",
    "        \n",
    "        # Make the number of additional pictures needed to arrive at the mean\n",
    "        for num in range(new_wanted):\n",
    "            \n",
    "            # Rotate images and append new ones to more_X, append the class to more_y\n",
    "            more_X.append(ndimage.rotate(X_train[picture][random.randint(0,pics_in_class[i] - 1)], random.choice(angles), reshape=False))\n",
    "            more_y.append(i)\n",
    "        \n",
    "        # Append the pictures generated for each class back to the original data\n",
    "        X_train = np.append(X_train, np.array(more_X), axis=0)\n",
    "        y_train = np.append(y_train, np.array(more_y), axis=0)\n",
    "        \n",
    "print('Additional data generated. Any classes lacking data now have', min_desired, 'pictures.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated number of training examples = 52396\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1621b3da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, bins = n_classes)\n",
    "\n",
    "updated_n_train = len(X_train)\n",
    "print(\"The updated number of training examples =\", updated_n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before split X_train: (39209, 32, 32, 1)\n",
      "before split y_train: (39209,)\n",
      "after split X_train: (35288, 32, 32, 1)\n",
      "after split y_train: (35288,)\n",
      "after split X_valid: (3921, 32, 32, 1)\n",
      "after split y_valid: (3921,)\n",
      "Dataset successfully split for training and validation.\n"
     ]
    }
   ],
   "source": [
    "# Splitting the training dataset into training and validation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle the data prior to splitting\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "print ('before split X_train:', X_train.shape)\n",
    "print ('before split y_train:', y_train.shape)\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, stratify = y_train, test_size=0.1, random_state=23)\n",
    "\n",
    "print ('after split X_train:', X_train.shape)\n",
    "print ('after split y_train:', y_train.shape)\n",
    "print ('after split X_valid:', X_valid.shape)\n",
    "print ('after split y_valid:', y_valid.shape)\n",
    "\n",
    "\n",
    "print('Dataset successfully split for training and validation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# The below is only necessary to reset if the notebook has not been shutdown\n",
    "tf.reset_default_graph()\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 150\n",
    "\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def neural_network(x):    \n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    # Weight and bias\n",
    "    # If not using grayscale, the third number in shape would be 3\n",
    "    c1_weight = tf.Variable(tf.truncated_normal(shape = (5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "    c1_bias = tf.Variable(tf.zeros(6))\n",
    "    # Apply convolution\n",
    "    conv_layer1 = tf.nn.conv2d(x, c1_weight, strides=[1, 1, 1, 1], padding='VALID') + c1_bias\n",
    "    print ('conv_layer1 ', conv_layer1.get_shape())\n",
    "    \n",
    "    \n",
    "    # Activation for layer 1\n",
    "    conv_layer1 = tf.nn.relu(conv_layer1)\n",
    "    \n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv_layer1 = tf.nn.avg_pool(conv_layer1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    print ('conv_layer_1 after avg_pool:', conv_layer1.get_shape())\n",
    "    \n",
    "    # Layer 2: Convolutional. Output = 10x10x16.\n",
    "    # Note: The second layer is implemented the exact same as layer one, with layer 1 as input instead of x\n",
    "    # And then of course changing the numbers to fit the desired ouput of 10x10x16\n",
    "    # Weight and bias\n",
    "    c2_weight = tf.Variable(tf.truncated_normal(shape = (5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    c2_bias = tf.Variable(tf.zeros(16))\n",
    "    # Apply convolution for layer 2\n",
    "    conv_layer2 = tf.nn.conv2d(conv_layer1, c2_weight, strides=[1, 1, 1, 1], padding='VALID') + c2_bias\n",
    "    \n",
    "    print ('conv_layer2 ', conv_layer2.get_shape())\n",
    "    \n",
    "    \n",
    "    # Activation for layer 2\n",
    "    conv_layer2 = tf.nn.relu(conv_layer2)\n",
    "    \n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv_layer2 = tf.nn.avg_pool(conv_layer2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    print ('conv_layer2 after avg_pool:',conv_layer2.get_shape() )\n",
    "    # Flatten to get to fully connected layers. Input = 5x5x16. Output = 400.\n",
    "    flat = tf.contrib.layers.flatten(conv_layer2)\n",
    "    \n",
    "    print('flat ',flat.get_shape())\n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    # Although this is fully connected, the weights and biases still are implemented similarly\n",
    "    # There is no filter this time, so shape only takes input and output\n",
    "    # Weight and bias\n",
    "    fc1_weight = tf.Variable(tf.truncated_normal(shape = (400, 200), mean = mu, stddev = sigma))\n",
    "    fc1_bias = tf.Variable(tf.zeros(200))\n",
    "    # Here is the main change versus a convolutional layer - matrix multiplication instead of 2D convolution\n",
    "    fc1 = tf.matmul(flat, fc1_weight) + fc1_bias\n",
    "    \n",
    "     \n",
    "    print ('fc1 ', fc1.get_shape())\n",
    "    \n",
    "    \n",
    "    # Activation for the first fully connected layer.\n",
    "    # Same thing as before\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Dropout, to prevent overfitting\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    print ('fc1 after dropout:', fc1.get_shape())\n",
    "    \n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    # Same as the fc1 layer, just with updated output numbers\n",
    "    fc2_weight = tf.Variable(tf.truncated_normal(shape = (200, 100), mean = mu, stddev = sigma))\n",
    "    fc2_bias = tf.Variable(tf.zeros(100))\n",
    "    # Again, matrix multiplication\n",
    "    fc2 = tf.matmul(fc1, fc2_weight) + fc2_bias\n",
    "    \n",
    "    print ('fc2 ', fc2.get_shape())\n",
    "    \n",
    "    \n",
    "    # Activation.\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # Dropout\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    print ('fc2 after dropout: ', fc2.get_shape())\n",
    "    \n",
    "    # Layer 5 Fully Connected. Input = 84. Output = 43.\n",
    "    # Since this is the final layer, output needs to match up with the number of classes\n",
    "    fc3_weight = tf.Variable(tf.truncated_normal(shape = (100, 43), mean = mu, stddev = sigma))\n",
    "    fc3_bias = tf.Variable(tf.zeros(43))\n",
    "    # Again, matrix multiplication\n",
    "    logits = tf.matmul(fc2, fc3_weight) + fc3_bias\n",
    "    print ('logits: ', logits.get_shape())\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set placeholder variables for x, y, and the keep_prob for dropout\n",
    "# Also, one-hot encode y\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer1  (?, 28, 28, 6)\n",
      "conv_layer_1 after avg_pool: (?, 14, 14, 6)\n",
      "conv_layer2  (?, 10, 10, 16)\n",
      "conv_layer2 after avg_pool: (?, 5, 5, 16)\n",
      "flat  (?, 400)\n",
      "fc1  (?, 200)\n",
      "fc1 after dropout: (?, 200)\n",
      "fc2  (?, 100)\n",
      "fc2 after dropout:  (?, 100)\n",
      "logits:  (?, 43)\n"
     ]
    }
   ],
   "source": [
    "# Setting learning rate, loss functions, and optimizer\n",
    "rate = 0.005\n",
    "\n",
    "logits = neural_network(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below is used in the validation part of the neural network\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy =  sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob : 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (35288, 32, 32, 1)\n",
      "X_valid shape: (3921, 32, 32, 1)\n",
      "y_valid shape: (3921,)\n",
      "y_train shape (35288,)\n",
      "logits shape: (?, 43)\n",
      "Training...\n",
      "\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)  batch_y.shape (150,)\n",
      "batch_x shape: (150, 32, 32, 1)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a58c8dd3a7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'batch_x shape:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' batch_y.shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dc/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dc/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mevent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mevent_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mevent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Train your model here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "save_file = './train_model'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    print ('X_train shape:', X_train.shape)\n",
    "    print('X_valid shape:', X_valid.shape)\n",
    "    print('y_valid shape:', y_valid.shape)\n",
    "    print ('y_train shape', y_train.shape)\n",
    "    \n",
    "    print ('logits shape:', logits.get_shape())\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            #print ('batch_x shape:',batch_x.shape,' batch_y.shape', batch_y.shape)\n",
    "            loss = sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob : 0.7})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    # Save the model\n",
    "    saver.save(sess, \"./saveme\")\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.931\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
